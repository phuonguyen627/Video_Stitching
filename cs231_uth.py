# -*- coding: utf-8 -*-
"""CS231_UTH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G3nU2_56V4dBpr3qMAI-zq9QR50OO-v-
"""

!pip uninstall imgaug
!pip install imgaug==0.2.5

pip install opencv-contrib-python==3.4.2.17

import cv2 
import numpy as np
import imutils
import tqdm
import os
from moviepy.editor import ImageSequenceClip

class VideoStitcher:
    def __init__(self, left_video_in_path, right_video_in_path, video_out_path, video_out_width=1200, display=False):
        self.left_video_in_path = left_video_in_path 
        self.right_video_in_path = right_video_in_path
        self.video_out_path = video_out_path
        self.video_out_width = video_out_width 
        self.display = display 
        self.saved_homo_matrix = None 
 
    def stitch(self, images, ratio=0.75, reproj_thresh=0): 
        (image_b, image_a) = images 

        if self.saved_homo_matrix is None:
            (keypoints_a, features_a) = self.detect_and_extract(image_a) 
            (keypoints_b, features_b) = self.detect_and_extract(image_b)

            matched_keypoints = self.match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh) 

            if matched_keypoints is None: 
                return None

            self.saved_homo_matrix = matched_keypoints[1] 

        output_shape = (image_a.shape[1] + image_b.shape[1], image_a.shape[0])  
        #output_shape = (1200, 540)
        result = cv2.warpPerspective(image_a, self.saved_homo_matrix, output_shape)
        result[0:image_b.shape[0], 0:image_b.shape[1]] = image_b

        return result

    @staticmethod
    def detect_and_extract(image): 
        descriptor = cv2.xfeatures2d.SIFT_create() 
        (keypoints, features) = descriptor.detectAndCompute(image, None) 
        keypoints = np.float32([keypoint.pt for keypoint in keypoints]) 

        return (keypoints, features) 

    @staticmethod
    def match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh): 

        matcher = cv2.DescriptorMatcher_create("BruteForce") 
        raw_matches = matcher.knnMatch(features_a, features_b, k=2) 
        matches = [] 

        for raw_match in raw_matches: 
            if len(raw_match) == 2 and raw_match[0].distance < raw_match[1].distance * ratio: 
                matches.append((raw_match[0].trainIdx, raw_match[0].queryIdx))

        if len(matches) > 4: 
            points_a = np.float32([keypoints_a[i] for (_, i) in matches]) 
            points_b = np.float32([keypoints_b[i] for (i, _) in matches]) 

            (homography_matrix, status) = cv2.findHomography(points_a, points_b, cv2.RANSAC, reproj_thresh) 
            return (matches, homography_matrix, status) 
        return None


    def run(self):
        left_video = cv2.VideoCapture(self.left_video_in_path) 
        right_video = cv2.VideoCapture(self.right_video_in_path)
        print('[INFO]: {} and {} loaded'.format(self.left_video_in_path.split('/')[-1],
                                                self.right_video_in_path.split('/')[-1]))
        print('[INFO]: Video stitching starting....')

        n_frames = min(int(left_video.get(cv2.CAP_PROP_FRAME_COUNT)),
                       int(right_video.get(cv2.CAP_PROP_FRAME_COUNT)))
        fps = int(left_video.get(cv2.CAP_PROP_FPS))
        frames = [] 

        for _ in tqdm.tqdm(np.arange(n_frames)):
            ok, left = left_video.read()  
            _, right = right_video.read()
            left = cv2.cvtColor(left, cv2.COLOR_BGR2RGB)
            right = cv2.cvtColor(right, cv2.COLOR_BGR2RGB)
            if ok:
                stitched_frame = self.stitch([left, right]) 

                if stitched_frame is None: 
                    print("[INFO]: Ma trận Homography không được tính!")
                    break

                # Add frame to video
                stitched_frame = imutils.resize(stitched_frame, width=self.video_out_width) 
                kernel = np.ones((3,3), np.float32)/9
                stitched_frame = cv2.filter2D(stitched_frame, -1, kernel)
                
                kernel = np.array([[-1,-1,-1], 
                   [-1, 9,-1],
                   [-1,-1,-1]])
                stitched_frame = cv2.filter2D(stitched_frame, -1, kernel)
                frames.append(stitched_frame)

                if self.display:
                    cv2.imshow("Result", stitched_frame) 

                if cv2.waitKey(1) & 0xFF == ord("q"):
                    break
        right_video.release()
        left_video.release()
        cv2.destroyAllWindows()
        print('[INFO]: Video stitching finished')

        print('[INFO]: Saving {} in {}'.format(self.video_out_path.split('/')[-1],
                                               os.path.dirname(self.video_out_path)))
        clip = ImageSequenceClip(frames, fps=fps) 
        clip.write_videofile(self.video_out_path, codec='mpeg4', audio=False, verbose=False) 
        print('[INFO]: {} saved'.format(self.video_out_path.split('/')[-1]))

# Example call to 'VideoStitcher'
stitcher = VideoStitcher(left_video_in_path='case1-Left.mp4', 
                         right_video_in_path='case1-Right.mp4',
                         video_out_path='case1-Center.mp4')
stitcher.run()

